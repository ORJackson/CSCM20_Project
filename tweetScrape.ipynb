{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will write the code to collect tweets between January 2019 and June 2022. I will save the tweets to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import json_normalize\n",
    "import pip._vendor.requests \n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# To get bearer token environment variable\n",
    "bearer_token = os.environ.get(\"BEARER-TOKEN\")\n",
    "\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/all\" \n",
    "\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    return r\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_endpoint(url, params):\n",
    "    response = pip._vendor.requests.request(\"GET\", search_url, auth=bearer_oauth, params=params)\n",
    "    sleep(3)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKeywordQueryString():\n",
    "    keyWordData = pd.read_csv('finalAsianKeywords.csv')\n",
    "    words = keyWordData['keyword'].tolist()\n",
    "    keywordQueryString = \"\"\n",
    "\n",
    "    for word in words:\n",
    "        if word != words[-1]:\n",
    "            keywordQueryString = keywordQueryString + word + \" OR \"\n",
    "        else:\n",
    "            keywordQueryString = keywordQueryString + word\n",
    "    return keywordQueryString\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def query_params(keywordQueryString, startTime, endTime, maxResults, nextToken):\n",
    "\n",
    "    queryString = f'({keywordQueryString}) lang:en -is:retweet place_country:gb'\n",
    "\n",
    "\n",
    "    return      {'query': queryString, \n",
    "                'start_time': startTime, \n",
    "                'end_time': endTime, \n",
    "                'max_results': maxResults,\n",
    "                'tweet.fields': 'text,created_at,id',                \n",
    "                'next_token': nextToken}\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTweetsToCsv(keywordQueryString, startTime, endTime, maxResults, csvString):\n",
    "    next_token = {}\n",
    "    finished = False\n",
    "    data = []\n",
    "    mostRecentDate = \"\"\n",
    "    while finished == False:\n",
    "        json_response = connect_to_endpoint(search_url, query_params(keywordQueryString, startTime, endTime, maxResults, next_token))\n",
    "        if 'data' in json_response:\n",
    "            for r in json_response['data']:\n",
    "                date = r['created_at']\n",
    "                tweetId = r['id']\n",
    "                tweetText = r['text']\n",
    "                if \"\\n\" in tweetText:\n",
    "                    tweetText = tweetText.replace('\\n', ' ')\n",
    "                result = [date, tweetId, tweetText]\n",
    "                data.append(result)\n",
    "                mostRecentDate = date\n",
    "        if 'next_token' in json_response['meta']:\n",
    "            # Save the token to use for next call\n",
    "            next_token = json_response['meta']['next_token']\n",
    "        else:\n",
    "            finished = True\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(csvString, index=False, header=False, mode='a')\n",
    "    print(\"most recent date: \" + mostRecentDate)\n",
    "    return mostRecentDate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def twitterScraperLoopUntilComplete(keywordQueryString, startTime, endTime, maxResults, csvString):\n",
    "    csvFile = open(csvString, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    csvWriter.writerow(['date', 'id', 'tweet'])\n",
    "    csvFile.close()\n",
    "    end = endTime\n",
    "    shortFormatStartString = startTime[0:10]\n",
    "    finished = False\n",
    "    while finished == False:\n",
    "        end = getTweetsToCsv(keywordQueryString, startTime, end, maxResults, csvString)\n",
    "        if shortFormatStartString == end[0:10]:\n",
    "            print(\"Reached end time\")\n",
    "            finished = True\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most recent date: 2019-01-01T00:14:27.000Z\n",
      "Reached end time\n"
     ]
    }
   ],
   "source": [
    "queryStringOfKeywords = getKeywordQueryString()\n",
    "startTime = '2019-01-01T00:00:00Z'\n",
    "endTime = '2022-06-01T00:00:00Z'\n",
    "maxResults = 500\n",
    "outputCsv = \"twitterDataOutput.csv\"\n",
    "\n",
    "\n",
    "twitterScraperLoopUntilComplete(queryStringOfKeywords, startTime, endTime, maxResults, outputCsv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms no two tweets are the same in output CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no duplicates in the data\n"
     ]
    }
   ],
   "source": [
    "def confirmNoDuplicateTweets(csvFile):\n",
    "    newTweetData = pd.read_csv(csvFile)\n",
    "    ids = newTweetData['id'].tolist()\n",
    "    if len(set(ids)) == len(ids):\n",
    "        print(\"There are no duplicates in the data\")\n",
    "    else:\n",
    "        print(\"There are duplicates in the data\")\n",
    "\n",
    "confirmNoDuplicateTweets(outputCsv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b04001ca54e5ffc9c42d9fa47127f5c745705fc157c2151e145f1f4ee061d594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
